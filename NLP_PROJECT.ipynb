{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ta2YfN7LiY01",
        "outputId": "4fb5d0ed-1f7d-4248-ea81-ef061e10f07e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.97      0.98      5868\n",
            "           1       0.97      0.98      0.98      5352\n",
            "\n",
            "    accuracy                           0.98     11220\n",
            "   macro avg       0.98      0.98      0.98     11220\n",
            "weighted avg       0.98      0.98      0.98     11220\n",
            "\n",
            "Decision Tree Classifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      5868\n",
            "           1       0.99      1.00      1.00      5352\n",
            "\n",
            "    accuracy                           1.00     11220\n",
            "   macro avg       1.00      1.00      1.00     11220\n",
            "weighted avg       1.00      1.00      1.00     11220\n",
            "\n",
            "Random Forest Classifier\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.98      0.99      5868\n",
            "           1       0.98      1.00      0.99      5352\n",
            "\n",
            "    accuracy                           0.99     11220\n",
            "   macro avg       0.99      0.99      0.99     11220\n",
            "weighted avg       0.99      0.99      0.99     11220\n",
            "\n",
            "Confusion Matrix for Logistic Regression:\n",
            " [[5698  170]\n",
            " [  96 5256]]\n",
            "Confusion Matrix for Decision Tree:\n",
            " [[5841   27]\n",
            " [  15 5337]]\n",
            "Confusion Matrix for Random Forest:\n",
            " [[5762  106]\n",
            " [  12 5340]]\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Load datasets\n",
        "data_Fake = pd.read_csv('/content/Fake.csv')\n",
        "data_true = pd.read_csv('/content/True.csv')\n",
        "\n",
        "# Add class labels: 0 for Fake, 1 for True\n",
        "data_Fake['class'] = 0\n",
        "data_true['class'] = 1\n",
        "\n",
        "# Extract the last 10 rows for manual testing\n",
        "data_fake_manual_testing = data_Fake.tail(10)\n",
        "data_true_manual_testing = data_true.tail(10)\n",
        "\n",
        "# Drop the last 10 rows from the datasets\n",
        "data_Fake = data_Fake.iloc[:-10]\n",
        "data_true = data_true.iloc[:-10]\n",
        "\n",
        "# Combine datasets\n",
        "data_merge = pd.concat([data_Fake, data_true], axis=0)\n",
        "\n",
        "# Drop unnecessary columns\n",
        "data = data_merge.drop(['title', 'subject', 'date'], axis=1)\n",
        "\n",
        "# Shuffle the data\n",
        "data = data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Text cleaning function\n",
        "def wordopt(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub(\"\\\\W\", \" \", text)\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub('<.*?>+', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub('\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Apply text cleaning\n",
        "data['text'] = data['text'].apply(wordopt)\n",
        "\n",
        "# Split features and labels\n",
        "x = data['text']\n",
        "y = data['class']\n",
        "\n",
        "# Stratified train-test split to handle class imbalance\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, stratify=y, random_state=42)\n",
        "\n",
        "# Vectorization using TF-IDF\n",
        "vectorization = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), stop_words='english')\n",
        "xv_train = vectorization.fit_transform(x_train)\n",
        "xv_test = vectorization.transform(x_test) #Limits features to 5000 most important terms, considers unigrams and bigrams, and removes common English stopwords.\n",
        "\n",
        "# Balance the dataset using SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "xv_train_resampled, y_train_resampled = smote.fit_resample(xv_train, y_train) #Synthetic Minority Oversampling Technique creates synthetic samples for the minority class (if the dataset is imbalanced)\n",
        "\n",
        "# Logistic Regression\n",
        "LR = LogisticRegression(C=0.1, random_state=42)\n",
        "LR.fit(xv_train_resampled, y_train_resampled)\n",
        "pred_lr = LR.predict(xv_test)\n",
        "print(\"Logistic Regression\")\n",
        "print(classification_report(y_test, pred_lr)) #Fits a Logistic Regression model with regularization (C=0.1) and prints its performance.\n",
        "\n",
        "# Decision Tree Classifier\n",
        "DT = DecisionTreeClassifier(max_depth=10, random_state=42)\n",
        "DT.fit(xv_train_resampled, y_train_resampled)\n",
        "pred_dt = DT.predict(xv_test)\n",
        "print(\"Decision Tree Classifier\")\n",
        "print(classification_report(y_test, pred_dt)) #Trains a Decision Tree with a maximum depth of 10 to prevent overfitting.\n",
        "\n",
        "# Random Forest Classifier\n",
        "RF = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)\n",
        "RF.fit(xv_train_resampled, y_train_resampled)\n",
        "pred_rf = RF.predict(xv_test)\n",
        "print(\"Random Forest Classifier\")\n",
        "print(classification_report(y_test, pred_rf)) #Helper function to convert numerical predictions to human-readable labels.\n",
        "\n",
        "# Confusion Matrices\n",
        "print(\"Confusion Matrix for Logistic Regression:\\n\", confusion_matrix(y_test, pred_lr))\n",
        "print(\"Confusion Matrix for Decision Tree:\\n\", confusion_matrix(y_test, pred_dt))\n",
        "print(\"Confusion Matrix for Random Forest:\\n\", confusion_matrix(y_test, pred_rf))\n",
        "\n",
        "# Manual testing function\n",
        "def output_label(n):\n",
        "    return \"Fake news\" if n == 0 else \"Not a fake news\"\n",
        "\n",
        "def manual_testing(news):\n",
        "    testing_news = {\"text\": [news]}\n",
        "    new_def_test = pd.DataFrame(testing_news)\n",
        "    new_def_test[\"text\"] = new_def_test[\"text\"].apply(wordopt)\n",
        "    new_x_test = new_def_test[\"text\"]\n",
        "    new_xv_test = vectorization.transform(new_x_test)\n",
        "\n",
        "    # Predictions from models\n",
        "    pred_LR = LR.predict(new_xv_test)\n",
        "    proba_LR = LR.predict_proba(new_xv_test)[:, 1]\n",
        "    pred_DT = DT.predict(new_xv_test)\n",
        "    pred_RF = RF.predict(new_xv_test)\n",
        "\n",
        "    # Adjusted threshold for Logistic Regression\n",
        "    threshold = 0.6\n",
        "    pred_adjusted_LR = (proba_LR >= threshold).astype(int)\n",
        "\n",
        "    print(\"\\nRaw Predictions:\")\n",
        "    print(f\"LR: {pred_LR[0]} (Probability: {proba_LR[0]:.2f})\")\n",
        "    print(f\"DT: {pred_DT[0]}\")\n",
        "    print(f\"RF: {pred_RF[0]}\")\n",
        "\n",
        "    print(\"\\nAdjusted LR Prediction: \", output_label(pred_adjusted_LR[0]))\n",
        "    return print(\n",
        "        f\"\\nFinal Predictions:\\n\"\n",
        "        f\"LR: {output_label(pred_LR[0])}\\n\"\n",
        "        f\"DT: {output_label(pred_DT[0])}\\n\"\n",
        "        f\"RF: {output_label(pred_RF[0])}\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input for manual testing\n",
        "news = str(input(\"Enter a news text: \"))\n",
        "manual_testing(news)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDqrJr_mihyE",
        "outputId": "3230cfb5-f057-4ee6-abaa-3ea367b9090e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a news text: house widens ethics probe to include farenthold campaign work\n",
            "\n",
            "Raw Predictions:\n",
            "LR: 0 (Probability: 0.45)\n",
            "DT: 0\n",
            "RF: 0\n",
            "\n",
            "Adjusted LR Prediction:  Fake news\n",
            "\n",
            "Final Predictions:\n",
            "LR: Fake news\n",
            "DT: Fake news\n",
            "RF: Fake news\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input for manual testing\n",
        "news = str(input(\"Enter a news text: \"))\n",
        "manual_testing(news)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Quk6n-Yojv1C",
        "outputId": "058ae16b-ee53-4562-da25-7fd6c18e53b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a news text: The U.S. House of Representatives gave final approval on Wednesday to a sweeping, debt-financed tax bill in a midday vote. It now will go to President Donald Trump to sign into law, although the timing of that was unclear. The Senate approved the bill early on Wednesday. Here are the key parts of the bill, representing the biggest overhaul of the U.S. tax code in more than 30 years. CORPORATE TAX RATE: Cuts corporate income tax rate permanently to 21 percent from 35 percent, as of Jan. 1, 2018. PASS-THROUGHS: Creates a 20 percent deduction for the first $315,000 of qualified business income for joint filers of pass-through businesses such as partnerships and sole proprietorships. For income above that threshold, the legislation phases in limits, producing an effective marginal tax rate of no more than 29.6 percent.   CORPORATE ALTERNATIVE MINIMUM TAX: Repeals the 20 percent corporate alternative minimum tax, set up to ensure profitable corporations pay at least some tax. TERRITORIAL SYSTEM: Exempts U.S. corporations from U.S. taxes on most future foreign profits, ending the present worldwide system of taxing profits of all U.S.-based corporations, no matter where they are earned. This would align the U.S. tax code with most other industrialized nations, undercut many offshore tax-dodging strategies and deliver to multinationals a goal they have pursued for years.  REPATRIATION: Sets a one-time mandatory tax of 8 percent on illiquid assets and 15.5 percent on cash and cash equivalents for about $2.6 trillion in U.S. business profits now held overseas. This foreign cash pile was created by a rule making foreign profits tax-deferred if they are not brought into the United States, or repatriated. That rule would be rendered obsolete by the territorial system. ANTI-BASE EROSION MEASURES: Prevents companies from shifting profits out of the United States to lower-tax jurisdictions abroad. Sets an alternative minimum tax on payments between U.S. corporations and foreign affiliates, and limits on shifting corporate income through transfers of intangible property, including patents. In combination, these measures with the repatriation and territorial system provisions, represent a dramatic overhaul of the U.S. tax system for multinationals. CAPITAL EXPENSING: Allows businesses to immediately write off, or expense, the full value of investments in new plant and equipment for five years, then gradually eliminates this 100 percent expensing over five years beginning in year six. Also makes changes to permit for more expensing by small businesses. INTEREST DEDUCTION LIMIT: Caps business deductions for debt interest payments at 30 percent of taxable income, regardless of deductions for depreciation, amortization or depletion.  CLEAN ENERGY: Preserves tax credits for producing electricity from wind, biomass, geothermal, solar, municipal waste and hydropower. CARRIED INTEREST: Leaves in place â€œcarried interestâ€ loophole for private equity fund managers and some hedge fund managers, despite pledges by Republicans including President Donald Trump to close it. These financiers can now claim a lower capital gains tax rate on much of their income from investments held more than a year. A new rule would extend that holding period to three years, putting the loophole out of reach for some fund managers but preserving its availability for many. BRACKETS: Maintains current seven tax brackets, but temporarily changes most income levels and rates for each one. For married couples filing jointly, effective Jan. 1, 2018 and ending in 2026, income tax would be: 10 percent up to $19,050, versus 10 percent up to $18,650 under existing law; 12 percent on $19,051 to $77,400, versus 15 percent on$18,651 to $75,900; 22 percent on $77,401 to $165,000, versus 25 percent on $75,901 to $153,100; 24 percent on $165,001 to $315,000, versus 28 percent on $153,101 to $233,350; 32 percent on $315,001 to $400,000, versus 33 percent on $233,351 to $416,700; 35 percent on $400,001 to $600,000, versus 35 percent on $416,701 to $470,700 37 percent above $600,000, versus 39.6 percent above$470,700. For single individuals, effective Jan. 1, 2018 and ending in 2026, income tax would be: 10 percent up to $9,525, versus 10 percent up to $9,325 under existing law; 12 percent from $9,526 to $38,700, versus 15 percent on $9,326 to $37,950; 22 percent on $38,701 to $82,500, versus 25 percent on $37,951 to $91,900; 24 percent on $82,501 to $157,500, versus 28 percent on $91,901 to $191,650; 32 percent on $157,501 to $200,000, versus 33 percent on $191,651 to $416,700; 35 percent on $200,001 to $500,000, versus 35 percent on $416,701 to $418,400; 37 percent above $500,000, versus 39.6 percent above $418,400. These brackets would expire after 2025. STANDARD DEDUCTION: In a change expected to end itemizing of deductions for millions of Americans, the bill for eight years beginning on Jan. 1, 2018, would increase the standard deduction - a fixed amount that can be subtracted from adjusted gross income to lower taxable income - to $12,000 from $6,350 for individuals, and to $24,000 from $12,700 for married couples. CHILD TAX CREDIT: Doubles the child tax credit to $2,000 per dependent child under age 17, with a refundable portion of $1,400. The refundable portion allows families to lower their tax bills to zero and receive a refund for the remaining value. PERSONAL EXEMPTION: Temporarily eliminates the $4,050 individual personal exemption. Under present law, taxpayers who earn below certain income caps can subtract this fixed dollar amount from their adjusted gross incomes to lower their taxable incomes. Generally, one exemption has been allowed per individual, spouse and child or other dependent. This would take effect Jan. 1, 2018, but then the personal exemption would return in 2026.  INDIVIDUAL ALTERNATIVE MINIMUM TAX: Leaves the AMT in place but temporarily changes it by raising its exemptions and phase-outs. That will mean fewer people will have to pay the tax, while those who still do will take a smaller hit from it. INHERITANCES: Raises the exemption for estate and gift taxes to $10 million from $5 million per person and indexes the new exemption level for inflation after 2011. That means even fewer Americans would pay the estate tax, but it would stay on the books. MORTGAGES: For residences bought from Jan. 1, 2018, through Dec. 31, 2025, the bill caps the deduction for mortgage interest at $750,000 in home loan value. After Dec. 31, 2025, the cap would revert to $1 million in loan value. Suspends the deduction for interest on home equity loans from Jan. 1, 2018 until 2026. MEDICAL EXPENSES: Temporarily expands the deductibility of out-of-pocket medical expenses through 2019. OBAMACARE MANDATE: Repeals federal fine imposed on Americans under Obamacare for not obtaining health insurance coverage, a change expected to undermine the 2010 healthcare law. ANWR DRILLING: Allows oil drilling in Alaskaâ€™s Arctic National Wildlife Refuge.\n",
            "\n",
            "Raw Predictions:\n",
            "LR: 1 (Probability: 0.84)\n",
            "DT: 0\n",
            "RF: 1\n",
            "\n",
            "Adjusted LR Prediction:  Not a fake news\n",
            "\n",
            "Final Predictions:\n",
            "LR: Not a fake news\n",
            "DT: Fake news\n",
            "RF: Not a fake news\n"
          ]
        }
      ]
    }
  ]
}